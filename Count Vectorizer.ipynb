{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe9b9632",
   "metadata": {},
   "source": [
    "# <b>Remove Stop words</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59fdcde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79990fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "735e4760",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ma', 'too', 'at', 'all', 'she', 'how', \"hasn't\", 'for', 'once', 'have', 'any', \"that'll\", 've', 'y', 'only', 'the', 'after', 'other', 'hasn', 'into', 'yourself', 'which', 'ain', \"isn't\", 'having', \"aren't\", 'should', 'isn', 'than', 'to', 'don', 'o', 'nor', \"weren't\", 'under', 'again', 'that', 'if', 'off', 'a', 'an', 'll', \"shouldn't\", \"needn't\", 'is', 'no', 'who', 'doing', 'over', 'very', 'aren', 'my', 'be', 'we', 'd', \"haven't\", 'theirs', 'has', \"mustn't\", \"won't\", 'needn', 'then', 'does', 'most', 'by', 'when', 'myself', 'mustn', 'them', 'during', \"you're\", 'down', 'your', 'won', \"wouldn't\", 'mightn', 'while', 'with', 'before', \"you'd\", 'some', 'this', 'but', 'in', \"you've\", 'was', 'below', 'up', 'it', 'her', 'against', 'shan', \"shan't\", 'ourselves', 'he', 'i', 'same', 'herself', \"she's\", \"hadn't\", 'these', 't', \"wasn't\", 'his', 'wasn', 'can', \"didn't\", 'and', 'yourselves', 'being', 'as', 'their', 'out', 'our', 'doesn', 'me', 'him', 'didn', 'why', 'above', 'wouldn', 'you', 'just', 'himself', 'hadn', 'both', 'are', 'because', 'here', 'each', 'what', 'hers', \"mightn't\", 'through', \"doesn't\", 'were', 'will', 'do', 'whom', 'between', \"it's\", 'those', 'of', 'weren', 'few', \"don't\", 'further', 'yours', 'itself', \"you'll\", 's', 'not', 'shouldn', 'from', 'am', 'ours', 'couldn', \"should've\", 'where', 'until', 'm', 'did', 'or', 'had', 'on', 'its', 'themselves', \"couldn't\", 'there', 'more', 'they', 'so', 'haven', 'such', 'now', 'been', 'own', 'about', 're'}\n"
     ]
    }
   ],
   "source": [
    "#to show stopwords in english \n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf8551f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Here', 'we', 'are', 'putting', 'in', 'our', 'efforts', 'to', 'understand', 'the', 'sentence']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Here we are putting in our efforts to understand the sentence\"\n",
    "\n",
    "token_list = sentence.split()\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f299be1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here putting efforts understand sentence "
     ]
    }
   ],
   "source": [
    "for token in token_list:\n",
    "    if token not in stop_words:\n",
    "        print(token,end=\" \")\n",
    "        \n",
    "#here all the stop words are removed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05183945",
   "metadata": {},
   "source": [
    "# <b>Count Vectorizer</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76fd0e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91782387",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = [\"I like Dog\",\n",
    "           \"I hate cat\",\n",
    "           \"I like dog and cat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6927782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4)\t1\n",
      "  (0, 2)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 1)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 2)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 0)\t1\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "vectorized_doc = vectorizer.fit_transform(document)\n",
    "print(vectorized_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3abfb434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 1]\n",
      " [0 1 0 1 0]\n",
      " [1 1 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "#above one is not readable\n",
    "print(vectorized_doc.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2788b2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'like': 4, 'dog': 2, 'hate': 3, 'cat': 1, 'and': 0}\n"
     ]
    }
   ],
   "source": [
    "#showing the words present in each index of vector\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8105d6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1]\n",
      " [1 0 1 0]\n",
      " [1 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "#romoving stop words\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "vectorized_doc = vectorizer.fit_transform(document)\n",
    "print(vectorized_doc.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47582a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'like': 3, 'dog': 1, 'hate': 2, 'cat': 0}\n"
     ]
    }
   ],
   "source": [
    "#showing the words present in each index of vector\n",
    "print(vectorizer.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d34c0a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'dog', 'hate', 'like']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c31e77",
   "metadata": {},
   "source": [
    "## Assignment for stop word understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9475411f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words , but don't remove \"when, what, where, How\" from the sentence (HW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff5392ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens are : ['what', 'the', 'damn', 'are', 'you', 'think', 'you', 'can', 'do', 'in', 'my', 'place']\n",
      " \n",
      "what damn think place "
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords #also download (nltk.download('stopwords'))\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"\\w+\")\n",
    "\n",
    "sentence = \"what the damn are you think  you can do in my place?\"\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(\"tokens are :\",tokens,end=\"\\n \\n\")\n",
    "\n",
    "Interogative_words = [\"when\",\"what\",\"where\",\"how\",\"why\",\"who\"]\n",
    "for token in tokens:\n",
    "    if token not in stop_words or token in Interogative_words:\n",
    "        print(token,end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d149d50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f91e35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
